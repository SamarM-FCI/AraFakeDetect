{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### trans to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from google\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "import os\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translateToEn(text):\n",
    "    translation = translator.translate(text, src='ar', dest='en')\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Hug face\n",
    "def transToEng(text) :\n",
    "    tokenizer.src_lang = \"ar\"\n",
    "    encoded_zh = tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "    translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"fakeData.csv\")\n",
    "df1 = df[:200]\n",
    "df2 = df[200:400]\n",
    "df3 = df[400:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part1\n",
    "output_file_path = 'tranP1.csv'\n",
    "df1['Translated_Text'] = df1['cleanText'].apply(translateToEn)\n",
    "df1.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "# part2\n",
    "output_file_path = 'tranP2.csv'\n",
    "df2['Translated_Text'] = df2['cleanText'].apply(translateToEn)\n",
    "df2.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "# part3\n",
    "output_file_path = 'tranP3.csv'\n",
    "df3['Translated_Text'] = df3['cleanText'].apply(translateToEn)\n",
    "df3.to_csv(output_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transToArab(text):\n",
    "    tokenizer.src_lang = \"en\"\n",
    "    encoded_zh = tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"ar\"))\n",
    "    translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "def translateToAr(text):   # worst output\n",
    "    translation = translator.translate(text, src='en', dest='ar')\n",
    "    return translation.text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"trans/tranP1.csv\")\n",
    "df2 = pd.read_csv(\"trans/tranP2.csv\")\n",
    "df3 = pd.read_csv(\"trans/tranP3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part1\n",
    "output_file_path = 'backtranP1.csv'\n",
    "df1['Translated_Text'] = df1['cleanText'].apply(transToArab)\n",
    "df1.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleanText</th>\n",
       "      <th>Translated_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مات 23 شخص في النرويج بعد تلقي لقاح بوست 51</td>\n",
       "      <td>['وفاة 23 شخصا في النرويج بعد تلقي لقاح بوست 51']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>حراموس الولائيون لاينفعهم لقاح الشيتان الأكببب...</td>\n",
       "      <td>['ولاية حراموس لا تستفيد منهم لقاح الشيطان الك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>كما حذر الرئيس من مخاطر التطعيم التطعيمات خطير...</td>\n",
       "      <td>['كما حذر الرئيس من مخاطر التطعيمات خطيرة، وأض...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>أفضل عقار لكورونا العقار الي انت فيه وأحسن لقا...</td>\n",
       "      <td>['أفضل دواء لكورونا دواء لك فيه وأفضل لقاح لكو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>إصابة 13 شخصا بشلل فى الوجه بعد تلقيهم لقاح بإ...</td>\n",
       "      <td>['13 شخصا مصابا بالشلل في الوجه بعد تلقيهم لقا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleanText  \\\n",
       "0        مات 23 شخص في النرويج بعد تلقي لقاح بوست 51   \n",
       "1  حراموس الولائيون لاينفعهم لقاح الشيتان الأكببب...   \n",
       "2  كما حذر الرئيس من مخاطر التطعيم التطعيمات خطير...   \n",
       "3  أفضل عقار لكورونا العقار الي انت فيه وأحسن لقا...   \n",
       "4  إصابة 13 شخصا بشلل فى الوجه بعد تلقيهم لقاح بإ...   \n",
       "\n",
       "                                     Translated_Text  \n",
       "0  ['وفاة 23 شخصا في النرويج بعد تلقي لقاح بوست 51']  \n",
       "1  ['ولاية حراموس لا تستفيد منهم لقاح الشيطان الك...  \n",
       "2  ['كما حذر الرئيس من مخاطر التطعيمات خطيرة، وأض...  \n",
       "3  ['أفضل دواء لكورونا دواء لك فيه وأفضل لقاح لكو...  \n",
       "4  ['13 شخصا مصابا بالشلل في الوجه بعد تلقيهم لقا...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('backtranP1.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# part3\n",
    "output_file_path = 'backtranP3.csv'\n",
    "df3['Translated_Text'] = df3['cleanText'].apply(transToArab)\n",
    "df3.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from translate lib\n",
    "from translate import Translator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def translate_text(text):\n",
    "    translator = Translator(to_lang='ar', from_lang='en')\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part2\n",
    "df2 = pd.read_csv('trans/tranP2.csv')\n",
    "output_file_path = 'back2.csv'\n",
    "df2['Translated_Text'] = df2['cleanText'].apply(translate_text)\n",
    "df2.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'backtranP3.csv'\n",
    "df3['Translated_Text'] = df3['cleanText'].apply(translateToAr)\n",
    "df3.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### synonumous replacement (english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\UG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\UG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('كما حذر الرئيس من مخاطر التطعيم التطعيمات خطيرة وأضاف ماجوفولي لو استطاع العلماء اختراعها لكانوا قد توصلوا إلى لقاح ضد الإيدز والسرطان والسل منذ فترة طويلة وحث الرئيس ماجوفولي التنزانيين على مواصلة اتخاذ الاحتياطات في الوقت نفسه أعلن أنه لن يكون هناك حجر صحي في البلاد',\n",
       " 'كما حذر الرئيس من مخاطر التطعيم التطعيمات خطيرة وأضاف ماجوفولي لو استطاع العلماء اختراعها لكانوا قد توصلوا إلى لقاح ضد الإيدز والسرطان والسل منذ فترة طويلة وحث الرئيس ماجوفولي التنزانيين على مواصلة اتخاذ الاحتياطات في الوقت نفسه أعلن أنه لن يكون هناك حجر صحي في البلاد')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def synonym_replacement(text, n=1):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    augmented_sentence = words.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        for j in range(len(words)):\n",
    "            synsets = wordnet.synsets(words[j])\n",
    "            if synsets:\n",
    "                synonym = random.choice(synsets).lemmas()[0].name()\n",
    "                augmented_sentence[j] = synonym\n",
    "    \n",
    "    augmented_text = ' '.join(augmented_sentence)\n",
    "    return augmented_text\n",
    "\n",
    "data = pd.read_csv(\"fakeData.csv\")\n",
    "data.cleanText[2], synonym_replacement(data.cleanText[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset saved to synEngP1.csv\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"trans/tranP1.csv\")\n",
    "column_name = 'Translated_Text'  \n",
    "n =1\n",
    "data[column_name] = data[column_name].apply(lambda x: synonym_replacement(x, n))\n",
    "\n",
    "# Save the augmented dataset to a new CSV file\n",
    "augmented_file = 'synEngP1.csv'\n",
    "data.to_csv(augmented_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'Augmented dataset saved to {augmented_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset saved to synEngP3.csv\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"trans/tranP3.csv\")\n",
    "column_name = 'Translated_Text'  \n",
    "n =1\n",
    "data[column_name] = data[column_name].apply(lambda x: synonym_replacement(x, n))\n",
    "\n",
    "# Save the augmented dataset to a new CSV file\n",
    "augmented_file = 'synEngP3.csv'\n",
    "data.to_csv(augmented_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'Augmented dataset saved to {augmented_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import os\n",
    "translator = Translator()\n",
    "\n",
    "def translateToAr(text):  \n",
    "    translation = translator.translate(text, src='en', dest='ar')\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to arabic\n",
    "df1 = pd.read_csv('synon/synEngP1.csv')\n",
    "# part1\n",
    "output_file_path = 'synArbP1.csv'\n",
    "df1['arbSyn_Text'] = df1['Translated_Text'].apply(translateToAr)\n",
    "df1.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part2\n",
    "df1 = pd.read_csv('synon/synEngP3.csv')\n",
    "output_file_path = 'synArbP3.csv'\n",
    "df1['arbSyn_Text'] = df1['Translated_Text'].apply(translateToAr)\n",
    "df1.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('كما حذر الرئيس من مخاطر التطعيم التطعيمات خطيرة وأضاف ماجوفولي لو استطاع العلماء اختراعها لكانوا قد توصلوا إلى لقاح ضد الإيدز والسرطان والسل منذ فترة طويلة وحث الرئيس ماجوفولي التنزانيين على مواصلة اتخاذ الاحتياطات في الوقت نفسه أعلن أنه لن يكون هناك حجر صحي في البلاد',\n",
       " 'The President_of_the_United_States besides warn of the risk of vaccination , the dangerous vaccination .',\n",
       " 'The President_of_the_united_states إلى جانب تحذير من خطر التطعيم والتطعيم الخطير.')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check <<<==========\n",
    "data = pd.read_csv(\"synon/synArbP1.csv\")\n",
    "data.cleanText[2] , data.Translated_Text[2], data.arbSyn_Text[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random deletetion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def SentProcessing(sent):\n",
    "    # Remove Arabic diacritics (tashkeel)\n",
    "    sent = re.sub('[\\u0617-\\u061A\\u064B-\\u0652]', '', sent)\n",
    "\n",
    "    # Remove Arabic punctuation\n",
    "    sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove extra whitespaces and convert to lowercase\n",
    "    sent = ' '.join(sent.split()).lower()\n",
    "\n",
    "    return sent\n",
    "\n",
    "import random\n",
    "\n",
    "def Random_Deletion_Arabic(sent, probability):\n",
    "    '''Randomly remove each word in the sentence with probability p'''\n",
    "    psent = SentProcessing(sent)\n",
    "    sent_list = psent.split()\n",
    "    \n",
    "    if len(sent_list) == 1:\n",
    "        return ''.join(sent_list)\n",
    "\n",
    "    new_sents = []\n",
    "    for word in sent_list:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > probability:\n",
    "            new_sents.append(word)\n",
    "\n",
    "    if len(new_sents) == 0:\n",
    "        rand_int = random.randint(0, len(sent_list) - 1)\n",
    "        return ''.join([sent_list[rand_int]])\n",
    "\n",
    "    return ' '.join(new_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file_path = \"fakeData.csv\"\n",
    "output_file_path = 'randDel_FD2.csv'  # Define the path for the augmented dataset\n",
    "\n",
    "# Load the dataset into a DataFrame (assuming it has a 'Text' column)\n",
    "data = pd.read_csv(input_file_path)\n",
    "\n",
    "# Define the probability for random deletion\n",
    "probability = 0.4  # Adjust the probability as needed\n",
    "\n",
    "# Create a list to store augmented sentences\n",
    "augmented_sentences = []\n",
    "\n",
    "# Iterate through the dataset and apply random deletion\n",
    "for index, row in data.iterrows():\n",
    "    arabic_text = row['cleanText']\n",
    "    augmented_text = Random_Deletion_Arabic(SentProcessing(arabic_text), probability)\n",
    "    augmented_sentences.append({'Original_Text': arabic_text, 'Augmented_Text': augmented_text})\n",
    "\n",
    "# Create a new DataFrame from the augmented data\n",
    "augmented_df = pd.DataFrame(augmented_sentences)\n",
    "\n",
    "# Save the augmented dataset to a CSV file\n",
    "augmented_df.to_csv(output_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def contains_arabic_and_english(text):\n",
    "    # Define patterns for Arabic and English words\n",
    "    arabic_pattern = re.compile(r'[\\u0600-\\u06FF]+')\n",
    "    english_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    \n",
    "    # Check if both patterns match the text\n",
    "    contains_arabic = bool(arabic_pattern.search(text))\n",
    "    contains_english = bool(english_pattern.search(text))\n",
    "    \n",
    "    return contains_arabic and contains_english\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"trans/backtranP3.csv\")\n",
    "data['contains_arabic_and_english'] = data['Translated_Text'].apply(contains_arabic_and_english)\n",
    "filtered_data = data[data['contains_arabic_and_english'] == False]\n",
    "filtered_data.to_csv('filtered_backtranP3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from list to str\n",
    "data = pd.read_csv(\"trans/backtran.csv\")\n",
    "column_name = 'Translated_Text'  # Replace with the name of the column containing the lists\n",
    "data[column_name] = data[column_name].str.strip(\"[]\").str.strip(\"''\")\n",
    "data.to_csv('modified_backtrans.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Define a function to perform random word swap\n",
    "def random_word_swap(text, n=2):\n",
    "    words = text.split()\n",
    "    if len(words) < 2:\n",
    "        return text\n",
    "    else:\n",
    "        for _ in range(n):\n",
    "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('randDelFinal.csv')\n",
    "data = data[0:1000]\n",
    "column_name = 'Augmented_Text' \n",
    "data[column_name] = data[column_name].apply(random_word_swap)\n",
    "\n",
    "# Save the augmented dataset to a new CSV file\n",
    "augmented_file = 'addPart.csv'\n",
    "data.to_csv(augmented_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'Augmented dataset saved to {augmented_file}')\n",
    "# join all this file in final dataset.csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
