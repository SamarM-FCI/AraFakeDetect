{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file in read mode\n",
    "with open('ArCovidVac.txt', encoding='utf-8') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to csv file\n",
    "\n",
    "with open('ArCovidVac.txt', 'r', encoding='utf-8-sig') as txtfile, open('AraCovidVac.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    txt_reader = csv.reader(txtfile, delimiter='\\t')\n",
    "    csv_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row in txt_reader:\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = pd.read_csv('synArbP1.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'].head(10) , data['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtag\n",
    "def removeHT(text):\n",
    "    tweet = re.sub(r'#\\S+', '', text)\n",
    "    return tweet\n",
    "\n",
    "data['Translated_Text']= data['Translated_Text'].apply(lambda x: removeHT(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURLs(text):\n",
    "    tweet = re.sub(r'http\\S+',' ', text)\n",
    "    tweet = re.sub('_',' ', text)\n",
    "    return tweet\n",
    "\n",
    "data['Translated_Text']= data['Translated_Text'].apply(lambda x: removeURLs(x))\n",
    "data['Translated_Text'] = data['Translated_Text'].apply(lambda x: removeURLs(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_identical_words(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "\n",
    "    # Remove consecutive identical words\n",
    "    filtered_words = [words[0]]  # Initialize with the first word\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] != words[i - 1]:\n",
    "            filtered_words.append(words[i])\n",
    "\n",
    "    # Recreate the sentence without consecutive identical words\n",
    "    result = ' '.join(filtered_words)\n",
    "\n",
    "    return result\n",
    "\n",
    "data['Translated_Text'] = data['Translated_Text'].apply(lambda x: remove_identical_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "#remove non word characters, &amp, new line indicators, convert to same case and translate\n",
    "def clean_text(tweet):\n",
    "    tweet = re.sub(r'&\\S+', ' ', tweet) # remove '&amp'\n",
    "    tweet = tweet.replace(\"\\\\n\",' ') # remove end of line signs '\\n'\n",
    "    tweet = re.sub(r'[^\\w\\s]',' ',tweet) # remove non word characters\n",
    "    tweet = re.sub(r'@\\w*', \" \", tweet) # remove usernames\n",
    "    tweet = re.sub(r'ههه+','', tweet) # remove هههههههههههههههههه\n",
    "    tweet = tweet.lower() #convert to lower case\n",
    "    #tweet = re.sub(r'[0-9]','',tweet) #remove numbers\n",
    "    tweet = re.sub(r'[a-zA-Z]', ' ', tweet)\n",
    "    emojis = re.compile(\"[\"\n",
    "                    u\"\\U0001F600-\\U0001F64F\"\n",
    "                    u\"\\U0001F300-\\U0001F5FF\"\n",
    "                    u\"\\U0001F680-\\U0001F6FF\"\n",
    "                    u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                    u\"\\U00002702-\\U000027B0\"\n",
    "                    u\"\\U000024C2-\\U0001F251\"\n",
    "                    \"]+\", flags=re.UNICODE)\n",
    "    tweet = emojis.sub(r'', tweet) if emojis.search(tweet) else tweet # remove emojis\n",
    "    return tweet\n",
    "\n",
    "\n",
    "\n",
    "entry = data.iloc[0]['Translated_Text']\n",
    "print('raw data sample: ', entry)\n",
    "print('after cleaning: ', clean_text(entry))\n",
    "\n",
    "data['clean_Translated_Text'] = data['Translated_Text'].apply(lambda x: clean_text(x))\n",
    "data['clean_Translated_Text'] = data['Translated_Text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing White Spaces\n",
    "data['clean_Translated_Text'] = data['clean_Translated_Text'].apply(lambda text: \" \".join(text.split()))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleanText'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([data['clean_Translated_Text'],data['Translated_Text']], axis=1)\n",
    "train_data.to_csv('synArbP1_prepro.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبًا بك في قطر\n",
      "معمول في قطر\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "dataset = [\"مرحبًا بك في قطر\", \"Hello, this is an English text.\", \"معمول في قطر\"]\n",
    "\n",
    "# Function to detect language and filter out Qatari dialect\n",
    "def is_qatari_dialect(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        # Adjust the language code for Qatari Arabic as needed\n",
    "        return language != 'ar'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Filter out Qatari dialect texts\n",
    "non_qatari_texts = [text for text in dataset if not is_qatari_dialect(text)]\n",
    "\n",
    "# Print the filtered texts\n",
    "for text in non_qatari_texts:\n",
    "    print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
